{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 05 - Parallel Computing, Reproducibility, and Containers\n",
    "\n",
    "### Instructions\n",
    "\n",
    "This quiz is based on the material covered in lectures 21 to 24. You may use\n",
    "any resources available to you, including the lecture notes and the internet.\n",
    "\n",
    "All the data required for this quiz can be found in the `data` folder within this repository. If you need to recreate the datasets, you can do so by running the Python script included in the `script-data-generation` folder. Please make sure that the following Python packages are installed:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas pyarrow dask dask-sql joblib SQLAlchemy\n",
    "```\n",
    "\n",
    "This notebook contains the questions you need to answer.\n",
    "If possible, please submit your answers as an `.html` file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 01 - Parallelising a Function with Joblib\n",
    "\n",
    "Use `joblib` to parallelise the computation of squaring numbers in a large array. Import the required packages and write code that uses four cores to parallelise the computation.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "numbers = np.arange(1000000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "numbers = np.arange(1000000)\n",
    "squared_numbers = Parallel(n_jobs=4)(delayed(square)(i) for i in numbers)\n",
    "print(squared_numbers[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 02 - Using Dask Arrays for Large Data\n",
    "\n",
    "Using Dask's `array` module, create a Dask array of random numbers with 10,000 rows and 10,000 columns. The array should be divided into chunks of 1,000 rows by 1,000 columns to enable efficient parallel computation. Populate the array with random numbers drawn from a normal distribution, where the mean is 0 and the standard deviation is 1. After creating the array, compute the mean, standard deviation, maximum, and minimum of the array using Dask's parallel computation capabilities. Use the `.compute()` method to execute the computations and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.00015590685662276692\n",
      "Standard Deviation: 0.9998619226472895\n",
      "Maximum: 5.616139893508699\n",
      "Minimum: -6.067905582126675\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "\n",
    "dask_array = da.random.normal(0, 1, size=(10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "mean = dask_array.mean().compute()\n",
    "std_dev = dask_array.std().compute()\n",
    "max_value = dask_array.max().compute()\n",
    "min_value = dask_array.min().compute()\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Maximum: {max_value}\")\n",
    "print(f\"Minimum: {min_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 03 - Dask DataFrame Operations with Parquet Files\n",
    "\n",
    "The `data` folder containts datasets for four countries—Brazil, India, UK, and USA—covering the years 1945 to 2023. Each country's data is stored in a separate Parquet file named after the country (`Brazil.parquet`, `India.parquet`, `UK.parquet`, `USA.parquet`). Each file contains the following columns:\n",
    "\n",
    "- `country` (string): The name of the country.\n",
    "- `year` (integer): The year of the record.\n",
    "- `gdp_per_capita` (float): The GDP per capita for that country and year.\n",
    "- `population` (integer): The population for that country and year.\n",
    "\n",
    "Using Dask's `dataframe` module, read _only the `country` and the `gdp_per_capita` columns_ from the Parquet files into a Dask DataFrame. Then, compute the mean and standard deviation of the GDP per capita for each country using Dask's parallel computation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean GDP per capita for each country:\n",
      "country\n",
      "Brazil     5496.292031\n",
      "India      1251.704443\n",
      "UK        27496.851363\n",
      "USA       40189.822290\n",
      "Name: gdp_per_capita, dtype: float64\n",
      "Standard Deviation of GDP per capita for each country:\n",
      "country\n",
      "Brazil     2682.494158\n",
      "India       456.525628\n",
      "UK        10607.858036\n",
      "USA       14892.455747\n",
      "Name: gdp_per_capita, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet('data/*.parquet', columns=['country', 'gdp_per_capita'])\n",
    "\n",
    "mean_gdp = df.groupby('country')['gdp_per_capita'].mean().compute()\n",
    "std_gdp = df.groupby('country')['gdp_per_capita'].std().compute()\n",
    "\n",
    "print(\"Mean GDP per capita for each country:\")\n",
    "print(mean_gdp)\n",
    "print(\"Standard Deviation of GDP per capita for each country:\")\n",
    "print(std_gdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 04 - Dask and SQL Queries\n",
    "\n",
    "Load the `data.csv` file into a Dask DataFrame and use the `dask_sql` package to perform a SQL query that selects the `country` and `gdp_per_capita` columns and filters the rows where `gdp_per_capita` is greater than 20000 in 2014. Display the results. Do not forget to register the Dask DataFrame as a SQL table with the `create_table` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    country  gdp_per_capita\n",
      "227      UK    40455.486012\n",
      "306     USA    65386.141694\n"
     ]
    }
   ],
   "source": [
    "from dask_sql import Context\n",
    "\n",
    "data_df = dd.read_csv('data/data.csv')\n",
    "\n",
    "c = Context()\n",
    "\n",
    "c.create_table('data_table', data_df)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT country, gdp_per_capita\n",
    "FROM data_table\n",
    "WHERE gdp_per_capita > 20000 AND year = 2014\n",
    "\"\"\"\n",
    "result = c.sql(query)\n",
    "\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 05 - Parallelising a Function with Dask Delayed\n",
    "\n",
    "Suppose we need to compute the sum of squares of numbers for large ranges. The function below calculates the sum of squares from `0` up to `n-1`. Modify the given `sum_of_squares` function to use Dask's `@delayed` decorator and compute the sum of squares for each number in the numbers list in parallel. Measure and print the total execution time for the parallel computation, and print the results for each input number (as indicated in the code).\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def sum_of_squares(n):\n",
    "    \"\"\"Compute the sum of squares from 0 to n-1.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "numbers = [100_000_000, 200_000_000, 300_000_000, 400_000_000]\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the computations serially\n",
    "results_serial = []\n",
    "for n in numbers:\n",
    "    result = sum_of_squares(n)\n",
    "    results_serial.append(result)\n",
    "    print(f\"Sum of squares up to {n}: {result}\")\n",
    "\n",
    "# Measure the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "serial_execution_time = end_time - start_time\n",
    "print(f\"\\nSerial execution time: {serial_execution_time:.2f} seconds\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squares up to 100000000: 333333328333333350000000\n",
      "Sum of squares up to 200000000: 2666666646666666700000000\n",
      "Sum of squares up to 300000000: 8999999955000000050000000\n",
      "Sum of squares up to 400000000: 21333333253333333400000000\n",
      "\n",
      "Parallel execution time: 99.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dask import delayed, compute\n",
    "\n",
    "@delayed\n",
    "def sum_of_squares(n):\n",
    "    \"\"\"Compute the sum of squares from 0 to n-1.\"\"\"\n",
    "    return sum(i * i for i in range(n))\n",
    "\n",
    "numbers = [100_000_000, 200_000_000, 300_000_000, 400_000_000]\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the computations in parallel\n",
    "results_parallel = [sum_of_squares(n) for n in numbers]\n",
    "results_parallel = compute(*results_parallel)\n",
    "\n",
    "# Measure the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total execution time\n",
    "parallel_execution_time = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "for n, result in zip(numbers, results_parallel):\n",
    "    print(f\"Sum of squares up to {n}: {result}\")\n",
    "\n",
    "print(f\"\\nParallel execution time: {parallel_execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 06 - Using `pip` and `requirements.txt` for Dependency Management\n",
    "\n",
    "Explain how you can use `pip` to manage dependencies in a Python project. Describe the process of generating a `requirements.txt` file from your current environment and how to use this file to install the same packages in another environment or on a different machine. Please comment your code to explain each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "source": [
    "pip is a Python package manager that lets you install and update a variety of different libraries. pip can be used to manage dependencies in a Python project by creating a requirements.txt file. This file will keep down which version of each library is being used, so it can be easily transfered to another environment.\n",
    "\n",
    "1. Install the required packages in your current environment using pip.\n",
    "pip install numpy pandas matplotlib\n",
    "\n",
    "2. Generate a requirements.txt file from your current environment.\n",
    "This file will list all the installed packages and their versions.\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "3. The requirements.txt file will look like this:\n",
    "numpy==1.21.2\n",
    "pandas==1.3.3\n",
    "matplotlib==3.4.3\n",
    "\n",
    "4. To install the same packages in another environment or on a different machine,\n",
    "you can use the requirements.txt file.\n",
    "python -m venv myenv\n",
    "\n",
    "5. Activate the virtual environment:\n",
    "myenv\\Scripts\\activate\n",
    "\n",
    "6. Use pip to install the packages listed in the `requirements.txt` file:\n",
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 07 - Creating and Sharing a Conda Environment\n",
    "\n",
    "Describe the steps to create a new Conda virtual environment named `qtm350` with Python 3.12 and install the packages `numpy`, `pandas`, and `matplotlib`. Explain how to export this environment to an `environment.yml` file and how someone else can recreate the same environment on their machine using this file. Please comment your code to explain each step. There is no need to run the code for this question, but you can do so if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "1. Create the Conda environment:\n",
    "    conda create --name qtm350 python=3.12\n",
    "\n",
    "2. Activate the environment:\n",
    "    conda activate qtm350\n",
    "\n",
    "3. Install the required packages:\n",
    "    conda install numpy pandas matplotlib\n",
    "\n",
    "4. Export the environment to an `environment.yml` file:\n",
    "    conda env export > environment.yml\n",
    "\n",
    "5. Recreate the environment on another machine:\n",
    "    conda env create -f environment.yml\n",
    "\n",
    "6. Activate the recreated environment:\n",
    "    conda activate qtm350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 08 - Writing a Simple Dockerfile\n",
    "\n",
    "Write a simple `Dockerfile` that creates a Docker image for a Python application. The application consists of a single Python script named `app.py` that prints \"Hello, World!\" when executed. The `Dockerfile` should use the official Python image as the base image and copy the `app.py` script into the image. When the container is run, it should execute the `app.py` script and print \"Hello, World!\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "dockerfile"
    }
   },
   "outputs": [],
   "source": [
    "FROM python:3.12\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY app.py .\n",
    "\n",
    "CMD [\"python\", \"app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 09 - Writing a Dockerfile to Install Software on a Base Image\n",
    "\n",
    "Create a Dockerfile that starts from an Ubuntu 24.04 base image and installs the following software:\n",
    "\n",
    "- Git version 2.43.0-1ubuntu7.1\n",
    "- SQLite version 3.45.1-1ubuntu2\n",
    "\n",
    "Ensure that you specify the exact versions of the packages. Include commands to clean up the package manager cache after installation to reduce the image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "dockerfile"
    }
   },
   "outputs": [],
   "source": [
    "FROM ubuntu:24.04\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y git=1:2.43.0-1ubuntu7.1 sqlite3=3.45.1-1ubuntu2 && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN git --version && sqlite3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Question 10 - Writing a Dockerfile to Install Python and Packages on Ubuntu\n",
    "\n",
    "Write a `Dockerfile` that starts from an Ubuntu 24.04 base image, installs Python 3.12 and `pip`, and then uses `pip` to install specific versions of `numpy` (1.26.4), `pandas` (2.2.2), and `matplotlib` (3.9.2). Ensure you include commands to clean up the package manager cache after installation to reduce the image size. Set up a working directory named `app/` and configure the container to start an interactive Python shell `python3` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "dockerfile"
    }
   },
   "outputs": [],
   "source": [
    "FROM ubuntu:24.04\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y python3.12 python3-pip && \\\n",
    "    apt-get clean && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN pip3 install numpy==1.26.4 pandas==2.2.2 matplotlib==3.9.2\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "CMD [\"python3\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
